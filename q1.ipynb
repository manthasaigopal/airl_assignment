{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "16x3_lJ7iWOn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMzTZRMSnnQ5"
      },
      "source": [
        "## Image Embedding\n",
        "\n",
        "\n",
        "1.   Patchify (using conv2d)\n",
        "2.   Prepend Cls token for every image\n",
        "1.   Add positional encodings\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q6clvrkljaO"
      },
      "outputs": [],
      "source": [
        "# class ImageEmbedding(nn.Module):\n",
        "#   def __init__ (self, img_size : int, in_channels : int = 3, embed_dim : int = 768, patch_size : int = 16):\n",
        "#     super().__init__()\n",
        "\n",
        "#     num_patches = (img_size // patch_size) ** 2 # assuming sq. images and patches\n",
        "#     self.patchify = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "#     self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
        "#     self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "\n",
        "#     nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "#     nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     batch_size = x.size(0)\n",
        "\n",
        "#     x = self.patchify(x) # (B x embed_dim x H/p x W/p)\n",
        "#     x = x.flatten(2, 3)  # (B x embed_dim x N), N -> number of patches\n",
        "#     x = x.transpose(1,2) # (B x N x embed_dim)\n",
        "\n",
        "#     cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
        "#     x = torch.cat((cls_token, x), dim=1) # (B x N+1 x embed_dim)\n",
        "\n",
        "#     x = x + self.pos_embedding\n",
        "#     return x\n",
        "\n",
        "\n",
        "class ImageEmbedding(nn.Module):\n",
        "  def __init__(self, img_size: int, in_channels: int = 3, embed_dim: int = 768, patch_size: int = 16):\n",
        "    super().__init__()\n",
        "\n",
        "    stride = 8  # hardcoded stride < patch_size → overlapping patches\n",
        "\n",
        "    # compute number of patches accounting for overlap\n",
        "    num_patches_h = (img_size - patch_size) // stride + 1\n",
        "    num_patches_w = (img_size - patch_size) // stride + 1\n",
        "    num_patches = num_patches_h * num_patches_w\n",
        "\n",
        "    # overlapping patch extraction\n",
        "    self.patchify = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=stride)\n",
        "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "    self.pos_embedding = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "\n",
        "    nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "    nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B = x.size(0)\n",
        "\n",
        "    x = self.patchify(x)              # (B, embed_dim, H', W')\n",
        "    x = x.flatten(2).transpose(1, 2)  # (B, N, embed_dim)\n",
        "\n",
        "    cls_token = self.cls_token.expand(B, -1, -1)\n",
        "    x = torch.cat((cls_token, x), dim=1)  # (B, N+1, embed_dim)\n",
        "\n",
        "    x = x + self.pos_embedding\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5-2pZe8xoMz"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m45bQjEIs9hK",
        "outputId": "2b8bcfbe-f439-4e11-d490-e2bb997dd8ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 197, 768])\n"
          ]
        }
      ],
      "source": [
        "ie = ImageEmbedding(224)\n",
        "\n",
        "x = torch.rand(16, 3, 224, 224)\n",
        "out = ie(x)\n",
        "\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKJKesAAB0Y1"
      },
      "source": [
        "## Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cdvai_GswTH1"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embed_dim : int = 768, num_heads : int = 12):\n",
        "    super().__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = embed_dim // num_heads\n",
        "\n",
        "    self.query_linear = nn.Linear(embed_dim, embed_dim, bias=False) # W_q\n",
        "    self.key_linear = nn.Linear(embed_dim, embed_dim, bias=False)   # W_k\n",
        "    self.value_linear = nn.Linear(embed_dim, embed_dim, bias=False) # W_v\n",
        "\n",
        "    self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    batch_size, seq_len, _ = x.size()\n",
        "    x = x.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "    x = x.permute(0, 2, 1, 3)\n",
        "    return x\n",
        "\n",
        "  def compute_attention(self, query, key, value):\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    return torch.matmul(attention_weights, value)\n",
        "\n",
        "  def combine_heads(self, x):\n",
        "    batch_size, _, seq_len, _ = x.size()\n",
        "    x = x.permute(0, 2, 1, 3).contiguous()\n",
        "    return x.view(batch_size, -1, self.embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    query = self.split_heads(self.query_linear(x))\n",
        "    key = self.split_heads(self.key_linear(x))\n",
        "    value = self.split_heads(self.value_linear(x))\n",
        "\n",
        "    attention_weights = self.compute_attention(query, key, value)\n",
        "    output = self.combine_heads(attention_weights)\n",
        "    return self.output_linear(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89-v9sHhzaXF"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbIEG3F2BEfG",
        "outputId": "c4d1663c-761a-40b5-9d78-46470e767b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input :  torch.Size([2, 4, 256])\n",
            "Output :  torch.Size([2, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "mha = MultiHeadAttention(embed_dim=256, num_heads=8)\n",
        "\n",
        "x = torch.randn(2, 4, 256)\n",
        "output = mha(x)\n",
        "\n",
        "print(\"Input : \", x.shape)\n",
        "print(\"Output : \", output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG5ZixFBzfqM"
      },
      "source": [
        "## Encoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8aXVp8mTzcha"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.2):\n",
        "    super().__init__()\n",
        "    self.multihead_attention = MultiHeadAttention(embed_dim, num_heads)\n",
        "    self.norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(embed_dim, mlp_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(mlp_dim, embed_dim)\n",
        "    )\n",
        "    self.norm2 = nn.LayerNorm(embed_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.dropout(self.multihead_attention(x))\n",
        "    x = x + self.dropout(self.mlp(self.norm1(x)))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG9tU8CL1KbY"
      },
      "source": [
        "## ViT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qAcjhtrk1J0I"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "  def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, num_heads, mlp_dim, depth=6, dropout=0.2):\n",
        "    super().__init__()\n",
        "    self.image_embeddings = ImageEmbedding(img_size, in_channels, embed_dim, patch_size)\n",
        "    self.encoder = nn.ModuleList([\n",
        "        EncoderBlock(embed_dim, num_heads, mlp_dim, dropout) for _ in range(depth)\n",
        "    ])\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "    self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.image_embeddings(x)\n",
        "    for block in self.encoder:\n",
        "      x = block(x)\n",
        "    x = self.norm(x)\n",
        "    cls_token = x[:, 0]\n",
        "    return self.head(cls_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0EdIy24yn4p",
        "outputId": "99fbc121-88a4-4797-de45-24702191037b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 10])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(2, 3, 32, 32)  # batch of 2, 32x32 images\n",
        "model = ViT(img_size=32, patch_size=8, in_channels=3, num_classes=10, embed_dim=64, depth=4, num_heads=4, mlp_dim=128)\n",
        "\n",
        "logits = model(x)\n",
        "print(logits.shape)  # (2, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORXWKfgpT0Ng"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfqYRVKf3clc",
        "outputId": "eb94dec8-6907-4053-a3aa-da7aee9de953"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u2Ee_YGT4jj"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL0DrDIq7QGL"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ViT(\n",
        "    img_size=224,\n",
        "    patch_size=16,\n",
        "    in_channels=3,\n",
        "    num_classes=10,\n",
        "    embed_dim=128,\n",
        "    num_heads=4,\n",
        "    mlp_dim=256,\n",
        "    depth=4,\n",
        "    dropout=0.5\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sIGg6AmR7WQ4"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)\n",
        "\n",
        "csv_file = \"training_log.csv\"\n",
        "with open(csv_file, mode=\"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"epoch\", \"train_loss\", \"train_acc\", \"test_acc\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1CqpewN7aqj",
        "outputId": "cc6a8199-3592-4371-8e13-c2b9e867dd3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10] Loss: 1.8418 Train Acc: 31.64% Test Acc: 39.53% (Best: 39.53%)\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 200\n",
        "best_test_acc = 0.0\n",
        "model_save_path = \"best_vit_cifar10.pth\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / len(trainloader)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "    model.eval()\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "    test_accuracy = 100 * correct_test / total_test\n",
        "\n",
        "    with open(csv_file, mode=\"a\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([epoch+1, train_loss, train_accuracy, test_accuracy])\n",
        "\n",
        "    if test_accuracy > best_test_acc:\n",
        "        best_test_acc = test_accuracy\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Loss: {train_loss:.4f} \"\n",
        "          f\"Train Acc: {train_accuracy:.2f}% \"\n",
        "          f\"Test Acc: {test_accuracy:.2f}% (Best: {best_test_acc:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFpZvl53CW9G"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
